{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4996e977-64bf-4cc6-9966-d535e4e65a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Facial Emotion Model\n",
    "\n",
    "The following is the code needed to run the facial emotion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea29f6b8-975d-4201-af51-68d63fe0eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b42aa103-ba41-4f8a-b448-c4eca4862cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs important facial features from the image\n",
    "\n",
    "facial_emotions = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "def get_facial_features(image, draw=False, static_image_mode=True):\n",
    "\n",
    "    # Read the input image\n",
    "    image_input_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extracts 3D face landmarks from the image by using machine learning to infer the 3D facial surface\n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=static_image_mode,\n",
    "                                                max_num_faces=1,\n",
    "                                                min_detection_confidence=0.5)\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    results = face_mesh.process(image_input_rgb)\n",
    "\n",
    "    image_landmarks = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        if draw:\n",
    "\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            mp_drawing_styles = mp.solutions.drawing_styles\n",
    "            drawing_spec = mp_drawing.DrawingSpec(thickness=2, circle_radius=1)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=results.multi_face_landmarks[0],\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec)\n",
    "\n",
    "        ls_single_face = results.multi_face_landmarks[0].landmark\n",
    "        xs_ = []\n",
    "        ys_ = []\n",
    "        zs_ = []\n",
    "        for idx in ls_single_face:\n",
    "            xs_.append(idx.x)\n",
    "            ys_.append(idx.y)\n",
    "            zs_.append(idx.z)\n",
    "        for j in range(len(xs_)):\n",
    "            image_landmarks.append(xs_[j] - min(xs_))\n",
    "            image_landmarks.append(ys_[j] - min(ys_))\n",
    "            image_landmarks.append(zs_[j] - min(zs_))\n",
    "    return image_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2aeafd5-3c96-48d5-abe8-6efc7f3c7cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded and saved in data.txt.\n"
     ]
    }
   ],
   "source": [
    "# Loads the data\n",
    "\n",
    "data_directory = \"./Testing-Faces\"\n",
    "\n",
    "output = []\n",
    "for emotion_index, emotion in enumerate(sorted(os.listdir(data_directory))):\n",
    "    for directory_image_path in os.listdir(os.path.join(data_directory, emotion)):\n",
    "        image_path = os.path.join(data_directory, emotion, directory_image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        facial_features = get_facial_features(image)\n",
    "        if len(facial_features) == 1404:\n",
    "            facial_features.append(int(emotion_index))\n",
    "            output.append(facial_features)\n",
    "\n",
    "np.savetxt('data.txt', np.asarray(output))\n",
    "print(\"Data has been loaded and saved in data.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d74f0c7-dfa3-4fdc-a163-e6c625348b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into x and y\n",
    "\n",
    "# Load data\n",
    "data_file = \"data.txt\"\n",
    "data = np.loadtxt(data_file)\n",
    "\n",
    "# Split data into features (X) and labels (Y)\n",
    "x_face = data[:, :-1]\n",
    "y_face = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2f2091-f020-4fff-bcab-4b2685e27992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.43%\n",
      "Confusion Matrix:\n",
      "           Anger  Disgust  Fear  Happiness  Neutral  Sadness  Surprise\n",
      "Anger          1        0     0          0        0        0         0\n",
      "Disgust        0        1     0          0        0        0         0\n",
      "Fear           0        0     0          0        0        0         1\n",
      "Happiness      0        0     0          1        0        0         0\n",
      "Neutral        0        0     0          0        1        0         0\n",
      "Sadness        0        0     0          0        1        0         0\n",
      "Surprise       0        0     0          0        0        0         1\n"
     ]
    }
   ],
   "source": [
    "# Load and test the model\n",
    "\n",
    "with open('./facial-emotion-model', 'rb') as f:\n",
    "    face_model = pickle.load(f)\n",
    "    \n",
    "# Evaluate the accuracy of the model\n",
    "y_predict = face_model.predict(x_face)\n",
    "accuracy = accuracy_score(y_face, y_predict)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "labels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']\n",
    "cm = confusion_matrix(y_face, y_predict)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31942a8-553e-48cb-bb0a-8071cb46a6e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pose Emotion Estimation\n",
    "\n",
    "The following is the code needed to run the pose emotion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155683a-1d59-4f24-a18b-7e3c89f78af7",
   "metadata": {},
   "source": [
    "Defining Classes for Dataset + ML usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21224c29-ee57-442f-b8b4-c7b5e145bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "\n",
    "set_seed(11) \n",
    "\n",
    "def extract_info_from_filename(filename, i):\n",
    "    \"\"\"\n",
    "    Extracts specific parts of the filename based on the provided index `i`.\n",
    "    The filename format is expected to follow: '[MF](actor_id)(emotion)(scenario_id)V(version).trc'.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The filename to parse.\n",
    "        i (int): The group index to extract (1 for actor_id, 2 for emotion, etc.).\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted part of the filename.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the filename format is not recognized.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    print(f\"Processing Filename: {filename}\")\n",
    "    \n",
    "    # Match the expected filename format\n",
    "    match = re.search(r'[MF](\\d+)([A-Za-z]{1,2})(\\d+)V(\\d+)\\.trc', filename)\n",
    "    if match:\n",
    "        extracted = match.group(i)  # Extract the specified group\n",
    "        print(f\"Extracted Group {i}: {extracted}\")\n",
    "        return extracted.lower()  # Convert to lowercase for consistency\n",
    "    else:\n",
    "        raise ValueError(f\"Filename format not recognized: {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_trc_files_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all .trc files from the specified directory.\n",
    "    \"\"\"\n",
    "    trc_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.trc')]\n",
    "    return trc_files\n",
    "\n",
    "# Step 2: Define Data Cleaning and Preprocessing\n",
    "def clean_and_parse(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    headers = lines[3].strip().split('\\t')[2:]\n",
    "    data = [line.strip().split('\\t') for line in lines[5:]]\n",
    "    \n",
    "    clean_data = []\n",
    "    for row in data:\n",
    "        try:\n",
    "            clean_row = [float(x) if x != '' else np.nan for x in row]\n",
    "            clean_data.append(clean_row)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    data_array = np.array(clean_data)\n",
    "    data_array = data_array[~np.isnan(data_array).any(axis=1)]  # 移除含 NaN 的行\n",
    "    return headers, data_array\n",
    "\n",
    "\n",
    "def prepare_stgcn_input(data_array, num_nodes=21):\n",
    "    \"\"\"\n",
    "    Prepares motion capture data for ST-GCN input format.\n",
    "    \"\"\"\n",
    "    num_frames, num_columns = data_array.shape\n",
    "    num_markers = num_columns // 3  # Each marker has X, Y, Z coordinates\n",
    "\n",
    "    # Validate the number of markers matches the expected nodes\n",
    "    if np.isnan(data_array).any() or np.isinf(data_array).any():\n",
    "        raise ValueError(\"Data contains NaN or Inf before reshaping.\")\n",
    "\n",
    "    # Reshape data into (T, V, C) where T = frames, V = nodes, C = coordinates (X, Y, Z)\n",
    "    reshaped_data = data_array[:, :num_nodes * 3].reshape(num_frames, num_nodes, 3)\n",
    "\n",
    "    # Transpose to match ST-GCN input format: (C, T, V, M)\n",
    "    # Assume a single person (M=1) in the scene\n",
    "    stgcn_input = np.transpose(reshaped_data, (2, 0, 1))  # (C=3, T, V)\n",
    "    return np.expand_dims(stgcn_input, axis=-1)  # Add M=1 dimension\n",
    "\n",
    "def preprocess_frames(data_array, target_frames=96):\n",
    "    \"\"\"\n",
    "    Preprocess the frames to ensure they are exactly `target_frames` long.\n",
    "    - If frames are 0, the file is skipped.\n",
    "    - If frames are less than `target_frames`, pad with zeros.\n",
    "    - If frames are greater than `target_frames`, truncate.\n",
    "    \"\"\"\n",
    "    num_frames = data_array.shape[0]\n",
    "    if num_frames == 0:\n",
    "        return None  # Skip files with 0 frames\n",
    "\n",
    "    # If frames are less than target, pad with zeros\n",
    "    if num_frames < target_frames:\n",
    "        padding = np.zeros((target_frames - num_frames, data_array.shape[1]))\n",
    "        data_array = np.vstack((data_array, padding))\n",
    "\n",
    "    # If frames are more than target, truncate\n",
    "    if num_frames > target_frames:\n",
    "        data_array = data_array[:target_frames, :]\n",
    "\n",
    "    return data_array\n",
    "\n",
    "class PoseDatasetWithLabelsFromDirectory(Dataset):\n",
    "    def __init__(self, directory_path, target_frames=96):\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "        file_paths = load_trc_files_from_directory(directory_path)\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            label = extract_info_from_filename(file_path, 2)\n",
    "            try:\n",
    "                _, data_array = clean_and_parse(file_path)\n",
    "                data_array = preprocess_frames(data_array, target_frames)\n",
    "                if data_array is not None:\n",
    "                    stgcn_input = prepare_stgcn_input(data_array)\n",
    "                    if not np.isnan(stgcn_input).any() and not np.isinf(stgcn_input).any():\n",
    "                        self.data_list.append(stgcn_input)\n",
    "                        self.labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"Invalid ST-GCN input for file: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data_list[idx], dtype=torch.float32),\n",
    "                self.labels[idx])\n",
    "\n",
    "# Step 4: Define ST-GCN Model\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, num_classes=7, num_nodes=21, input_channels=3):\n",
    "        super(STGCN, self).__init__()\n",
    "        # Spatial-temporal convolution layers\n",
    "        self.spatial_conv = nn.Conv2d(input_channels, 64, kernel_size=(1, num_nodes))\n",
    "        self.temporal_conv = nn.Conv2d(64, 128, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_conv(x)  # Spatial convolution\n",
    "        x = torch.relu(x)\n",
    "        x = self.temporal_conv(x)  # Temporal convolution\n",
    "        x = torch.relu(x)\n",
    "        x = torch.mean(x, dim=(-1, -2))  # Global average pooling\n",
    "        x = self.fc(x)  # Fully connected layer for classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf4403-29e4-4032-9dd4-e6daca67486e",
   "metadata": {},
   "source": [
    "Loading the Test Dataset (not seen by model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c930ca-3293-42e3-a371-d2781f97a46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A0V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A0V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A1V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A2V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A3V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A4V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A4V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A5V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A5V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D0V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D1V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D2V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D3V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D3V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D4V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D4V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D5V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F0V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F1V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F1V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F2V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F3V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F4V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F4V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F5V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F5V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V4.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H2V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H2V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H3V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H3V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H4V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H5V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H5V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N1V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N1V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N2V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N3V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N4V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N5V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V3.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA1V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA2V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA3V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA3V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA4V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA4V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA5V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU0V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU1V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU2V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU2V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU3V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU4V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU4V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU5V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A0V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A0V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A1V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A1V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A2V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A2V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A3V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A3V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A4V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A5V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V3.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D1V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D1V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V3.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D3V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D4V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V3.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F1V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F1V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F2V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F3V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F4V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F5V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H0V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H1V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H2V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H4V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H5V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N1V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N1V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N2V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N2V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V3.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N4V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N5V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA0V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V3.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA2V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA2V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA3V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA3V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA4V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA5V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA5V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU0V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU1V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU1V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU2V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V3.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V4.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU4V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU5V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU5V2.trc\n",
      "Extracted Group 2: SU\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: n\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: su\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: su\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: a\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: d\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: f\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: h\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: n\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: n\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: n\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: sa\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: su\n",
      "Data Shape: torch.Size([3, 96, 21, 1]), Label: su\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A0V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A0V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A1V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A2V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A3V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A4V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A4V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A5V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08A5V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D0V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D1V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D2V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D3V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D3V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D4V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D4V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08D5V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F0V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F1V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F1V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F2V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F3V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F4V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F4V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F5V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08F5V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H0V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H1V4.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H2V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H2V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H3V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H3V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H4V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H5V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08H5V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N1V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N1V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N2V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N3V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N4V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08N5V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA0V3.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA1V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA2V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA3V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA3V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA4V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA4V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SA5V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU0V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU1V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU2V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU2V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU3V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU4V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU4V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F08SU5V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A0V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A0V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A1V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A1V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A2V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A2V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A3V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A3V2.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A4V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09A5V1.trc\n",
      "Extracted Group 2: A\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D0V3.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D1V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D1V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D2V3.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D3V2.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09D4V1.trc\n",
      "Extracted Group 2: D\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F0V3.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F1V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F1V2.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F2V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F3V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F4V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09F5V1.trc\n",
      "Extracted Group 2: F\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H0V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H1V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H2V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V2.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H3V3.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H4V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09H5V1.trc\n",
      "Extracted Group 2: H\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N1V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N1V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N2V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N2V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V2.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N3V3.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N4V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09N5V1.trc\n",
      "Extracted Group 2: N\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA0V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA1V3.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA2V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA2V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA3V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA3V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA4V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA5V1.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SA5V2.trc\n",
      "Extracted Group 2: SA\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU0V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU1V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU1V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU2V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V2.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V3.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU3V4.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU4V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU5V1.trc\n",
      "Extracted Group 2: SU\n",
      "Processing Filename: ./Pose_Test_Data/trc_data\\F09SU5V2.trc\n",
      "Extracted Group 2: SU\n"
     ]
    }
   ],
   "source": [
    "file_paths = './Pose_Test_Data/trc_data' ## F08 & F09\n",
    "dataset_with_labels = PoseDatasetWithLabelsFromDirectory(file_paths)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_with_labels, batch_size=2, shuffle=False)\n",
    "\n",
    "# Debug dataset contents\n",
    "for i in range(len(dataset_with_labels)):\n",
    "    inputs, label = dataset_with_labels[i]\n",
    "    print(f\"Data Shape: {inputs.shape}, Label: {label}\")\n",
    "dataset_with_labels = PoseDatasetWithLabelsFromDirectory(file_paths)\n",
    "dataloader = DataLoader(dataset_with_labels, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464686da-fe22-4f20-82a2-703322cc957b",
   "metadata": {},
   "source": [
    "Loading in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7510ad44-9295-4f54-bd11-90aed14bb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = STGCN(num_classes=7)\n",
    "pose_model.load_state_dict(torch.load('./best_model.pth', weights_only=True))\n",
    "pose_model.eval()\n",
    "\n",
    "label_mapping = {\n",
    "    'h': 0, 'n': 1, 'su': 2, 'd': 3, 'f': 4, 'a': 5, 'sa': 6\n",
    "}\n",
    "\n",
    "# Finds first data point with the label \n",
    "desired_emotion = \"su\"\n",
    "\n",
    "for i in range(len(dataset_with_labels)):\n",
    "    inputs, label = dataset_with_labels[i]\n",
    "    if label == desired_emotion:\n",
    "        desired_pose = inputs\n",
    "        #print(f\"Found pose with {desired_emotion}: setting as desired_pose variable\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9350730a-30c0-47b6-89bb-4e13f709b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds first data point with the label\n",
    "def find_desired_pose(desired_emotion):\n",
    "    for i in range(len(dataset_with_labels)):\n",
    "        inputs, label = dataset_with_labels[i]\n",
    "        if label == desired_emotion:\n",
    "            desired_pose = inputs\n",
    "            #print(f\"Found pose with {desired_emotion}: setting as desired_pose variable\")\n",
    "            break\n",
    "    return desired_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1411d14e-dbb9-4e8a-b8de-c7faac251daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming inputs is a single input tensor and target is a single label\n",
    "\n",
    "pose_emotions = [\"Happiness\", \"Neutral\", \"Surprise\", \"Disgust\", \"Fear\", \"Anger\", \"Sadness\"]\n",
    "\n",
    "def test_pose_emotion(desired_emotion):\n",
    "    input_tensor = find_desired_pose(desired_emotion)  # Single input\n",
    "    target = desired_emotion  # Single target label\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.unsqueeze(0)  \n",
    "        \n",
    "        outputs = pose_model(input_tensor.squeeze(-1))\n",
    "    \n",
    "        try:\n",
    "            label_index = label_mapping[target]\n",
    "        except KeyError:\n",
    "            print(f\"Label {target} not found in mapping.\")\n",
    "            label_index = None\n",
    "        \n",
    "        if label_index is not None:\n",
    "            # Get prediction\n",
    "            prediction = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Check correctness\n",
    "            is_correct = (prediction == label_index)\n",
    "\n",
    "            print(f\"Body Pose Prediction: {pose_emotions[prediction.item()]}\")\n",
    "    return pose_emotions[prediction.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd3e7b-115d-4c4a-a75c-76e4db5a5fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NLP Emotion Model\n",
    "The following is the code needed to run the NLP emotion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009569b7-1755-4d57-a3ad-0c78cfdd3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved model\n",
    "with open('./nlp-model', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('./tokenizer', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# # Load the label encoder\n",
    "# with open('./label-encoder', 'rb') as f:\n",
    "#     label_encoder = pickle.load(f)\n",
    "\n",
    "# Load max_length (you would have saved this during training)\n",
    "with open('./max-length', 'rb') as f:\n",
    "    max_length = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af5306f-8a18-4436-b636-d7c3f45bfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_emotions = [\"Neutral\", \"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    # Convert the sentence to a sequence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    \n",
    "    # Pad the sequence to match the training data length\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = loaded_model.predict(padded_sequence)\n",
    "    #print(\"Prediction:\", prediction)\n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    # Convert back to original label\n",
    "    predicted_label = nlp_emotions[predicted_class]\n",
    "    \n",
    "    return predicted_label, prediction[0]\n",
    "\n",
    "# Example usage\n",
    "# sentence = \"Brooke tweeted 'I feel so joyful! Isn't this amazing?'\"\n",
    "# label, probabilities = predict_sentiment(sentence)\n",
    "# print(f\"Predicted Sentiment: {label}\")\n",
    "# print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450bb518-d906-4343-8aeb-2b7b1963bcac",
   "metadata": {},
   "source": [
    "## Combined Model - Evaluating All Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e49ad0a-4db2-46cb-9749-8fddd718e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Anger\n",
      "Facial Emotion Prediction: Anger\n",
      "Body Pose Prediction: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "NLP Emotion Prediction: Anger\n",
      "Combined Model Prediction: Anger\n"
     ]
    }
   ],
   "source": [
    "#### Anger\n",
    "\n",
    "print(\"True Emotion: Anger\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[0]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"a\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Bob tweeted 'I'm really aggravated by Ella because she eats cheese'\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a20b7f-b405-4e2f-b024-75a97c5f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Disgust\n",
      "Facial Emotion Prediction: Disgust\n",
      "Body Pose Prediction: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "NLP Emotion Prediction: Sadness\n",
      "Combined Model Prediction: Disgust\n"
     ]
    }
   ],
   "source": [
    "#### Disgust\n",
    "\n",
    "print(\"True Emotion: Disgust\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[1]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"d\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Brooke tweeted 'I'm so grossed out. Eww!! What a horrible smell!'\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46d10067-b762-4a58-a639-d28c4afb2d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Fear\n",
      "Facial Emotion Prediction: Surprise\n",
      "Body Pose Prediction: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "NLP Emotion Prediction: Sadness\n",
      "Combined Model Prediction: Surprise\n"
     ]
    }
   ],
   "source": [
    "#### Fear\n",
    "\n",
    "print(\"True Emotion: Fear\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[2]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"f\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: That movie was terrifying! I almost screamed. I was so scared\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f497e172-f489-4c53-8770-28cceec47382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Happiness\n",
      "Facial Emotion Prediction: Happiness\n",
      "Body Pose Prediction: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "NLP Emotion Prediction: Happiness\n",
      "Combined Model Prediction: Happiness\n"
     ]
    }
   ],
   "source": [
    "#### Happiness\n",
    "\n",
    "print(\"True Emotion: Happiness\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[3]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"h\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: Today we choose gratitude because we've realized somehow that when we count our blessings, we are happier here and now...\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60b7bace-7398-496b-93ad-e89eb489a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Neutral\n",
      "Facial Emotion Prediction: Neutral\n",
      "Body Pose Prediction: Fear\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "NLP Emotion Prediction: Neutral\n",
      "Combined Model Prediction: Neutral\n"
     ]
    }
   ],
   "source": [
    "#### Neutral\n",
    "\n",
    "print(\"True Emotion: Neutral\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[4]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"n\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: Whateves\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4205e89-ffb0-4c56-95dd-e1ca7cb87885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Sadness\n",
      "Facial Emotion Prediction: Neutral\n",
      "Body Pose Prediction: Sadness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "NLP Emotion Prediction: Sadness\n",
      "Combined Model Prediction: Sadness\n"
     ]
    }
   ],
   "source": [
    "#### Sadness\n",
    "\n",
    "print(\"True Emotion: Sadness\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[5]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"sa\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: Today was really depressing. I fell down the stairs. I can't stop crying.\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8e90156-f8ca-4c24-a0ad-9a1713f8821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Surprise\n",
      "Facial Emotion Prediction: Surprise\n",
      "Body Pose Prediction: Sadness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "NLP Emotion Prediction: Sadness\n",
      "Combined Model Prediction: Sadness\n"
     ]
    }
   ],
   "source": [
    "#### Surprise\n",
    "\n",
    "print(\"True Emotion: Surprise\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[6]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"su\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: Today my sister surprised me for my birthday! I was so shocked when I saw her. I still can't believe it\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "991d50e3-a89e-4559-bd45-17793eedcf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Emotion: Anger\n",
      "Facial Emotion Prediction: Anger\n",
      "Body Pose Prediction: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "NLP Emotion Prediction: Sadness\n",
      "Combined Model Prediction: Anger\n"
     ]
    }
   ],
   "source": [
    "#### Sarcasm - \"happy\" speech but angry expression/body pose -> Should get a true emotion of anger\n",
    "\n",
    "print(\"True Emotion: Anger\")\n",
    "\n",
    "# Face Emotion Model\n",
    "\n",
    "face_emotion_predict = face_model.predict(np.array(x_face[0]).reshape(1, -1))\n",
    "face_emotion_prediction = facial_emotions[int(face_emotion_predict[0])]\n",
    "print(\"Facial Emotion Prediction:\", face_emotion_prediction)\n",
    "\n",
    "# Body Pose Model\n",
    "\n",
    "pose_emotion = test_pose_emotion(\"a\")\n",
    "\n",
    "# NLP Model\n",
    "\n",
    "sentence = \"Tweet: So happy and positive all the time\"\n",
    "label, probabilities = predict_sentiment(sentence)\n",
    "print(f\"NLP Emotion Prediction: {label}\")\n",
    "\n",
    "# Combined Model\n",
    "\n",
    "emotions = [face_emotion_prediction, pose_emotion, label]\n",
    "\n",
    "emotion_counts = {}\n",
    "for emotion in emotions:\n",
    "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "\n",
    "max_count = max(emotion_counts.values())\n",
    "\n",
    "majority_emotions = [emotion for emotion, count in emotion_counts.items() if count == max_count]\n",
    "\n",
    "if len(majority_emotions) == 1:\n",
    "    print(\"Combined Model Prediction:\", majority_emotions[0])\n",
    "else:\n",
    "    print(\"Combined Model Prediction:\", face_emotion_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
